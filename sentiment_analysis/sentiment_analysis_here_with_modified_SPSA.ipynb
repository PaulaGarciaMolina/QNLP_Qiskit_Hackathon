{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artificial-allen",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "Sentiment analysis is a NLP technique to determine the sentiment of a sentence. In this implementation, we will focus on determining the sentiment of the subject of the subject or object or the sentence.\n",
    "\n",
    "Here I propose some possible sentences for our dataset, indicating where the meaning is encoded.\n",
    "\n",
    "- Morose man cries. (morose)\n",
    "- Irascible woman shouts. (irascible)\n",
    "- Frightened woman shouts. (frightened)\n",
    "- Joyful kid laughs. (joyful, laughs)\n",
    "- Furious man snaps. (furious, snaps)\n",
    "- Kid startles man. (startles)\n",
    "- Woman grieves man. (grieves)\n",
    "...\n",
    "\n",
    "\n",
    "| Nouns | Verbs | Adjectives |\n",
    "| --- | --- | --- |\n",
    "| Man | cries | morose |\n",
    "| Woman | laughs | irascible |\n",
    "| Kid | shouts | frightened |\n",
    "|  | snaps | cheerful |\n",
    "|  | entertains | gloomy |\n",
    "|  | grieves | furious |\n",
    "|  | startles | terrified |\n",
    "|  | irritates |joyful|\n",
    "\n",
    "The proposed vocabulary has 19 words and two different kind of sentences:\n",
    "- Adj + Subject + Intransitive verb\n",
    "- Subject + Transitive verb + Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-league",
   "metadata": {},
   "source": [
    "## 1. Create dataset\n",
    "\n",
    "The first step is to create a dataset using the presented vocabulary. We have to store the words in DisCoPy's Word objects, encoding their meaning (name) and their grammar (codomain). Then, we introduce the grammar of the allowed sentences and create all the possible grammatical sentences. The next step is to assign a sentiment to each sentence. However, there are sentences that although they are grammatically correct, their meaning makes no sense, so we would have to remove them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-picking",
   "metadata": {},
   "source": [
    "### 1.1. Define the vocabulary\n",
    "\n",
    "The first step to create a dataset is to define the words, their meaning and the word type. We have four three types of words: nouns, adjectives, verbs. We also distinguish two different types of verbs: transitive and intransitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ranking-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb69310-bf5a-4107-bafd-2b0558b48ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************************************************************\n",
    "# Fix settings concerning the ansatz, optimisation and backend\n",
    "#*****************************************************************************************************\n",
    "\n",
    "from time import time \n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "from discopy import Ty, Id, Box, Diagram, Word # Ty grammatical type for Word, Id identity line for \n",
    "# grammar, Box and Diagram for grammar, Word to encode the meaning of a word\n",
    "from discopy.rigid import Cup, Cap, Functor, Swap # Cup and Cap to connect words, Functor to translate\n",
    "# sentences to circuits, Swap operation\n",
    "from discopy.quantum.circuit import bit, qubit\n",
    "from discopy.quantum import Measure\n",
    "from discopy.quantum.tk import to_tk # to_tk function to transform a DisCoPy circuit to a pytket one\n",
    "from discopy.quantum.tk import Circuit as tk_Circuit_inDCP\n",
    "\n",
    "from qiskit import IBMQ\n",
    "from pytket.extensions.qiskit import AerBackend, IBMQBackend, IBMQEmulatorBackend\n",
    "from pytket import Circuit as tk_Circuit\n",
    "\n",
    "#-----------------------------\n",
    "# atomic pregroup grammar types\n",
    "#-----------------------------\n",
    "s, n = Ty('S'), Ty('N')\n",
    "\n",
    "#----------------------------------------\n",
    "# settings concerning the ansaetze\n",
    "#----------------------------------------\n",
    "q_s = 1        # number of qubits for sentence type s\n",
    "q_n = 1        # number of qubits for noun type n\n",
    "depth = 1      # number of IQP layers for non-single-qubit words\n",
    "p_n = 3        # number of parameters for a single-qubit word (noun); valued in {1,2,3}.\n",
    "\n",
    "#----------------------------------------\n",
    "# Parameters concerning the optimisation\n",
    "#----------------------------------------\n",
    "n_runs = 1      # number of runs over training procedure\n",
    "niter  = 200    # number of iterations for any optimisation run of training.\n",
    "\n",
    "#----------------------------------------\n",
    "# Parameters for quantum computation\n",
    "#----------------------------------------\n",
    "max_n_shots = 2 ** 13  # maximum shots possible\n",
    "\n",
    "#---------------------\n",
    "# Fix the backend\n",
    "#---------------------\n",
    "backend = AerBackend()  # this is a noise free quantum simulation that will be carried out on your computer\n",
    "                        # and which does not rely on an IBMQ account.\n",
    "\n",
    "# Alternatively: \n",
    "# ***************      !!! Note: Insert here your IBMQ account token !!!\n",
    "# provider = IBMQ.enable_account(<INSERT_IBM_QUANTUM_EXPERIENCE_TOKEN>)\n",
    "\n",
    "# backend = IBMQEmulatorBackend(<backend_name>, <credentials>)\n",
    "#or\n",
    "# backend = IBMQBackend(<backend_name>, <credentials>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handled-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, n = Ty('s'), Ty('n') # Define the types s and n\n",
    "nphr, adj, tv, iv, vphr = Ty('NP'), Ty('ADJ'), Ty('TV'), Ty('IV'), Ty('VP')\n",
    "\n",
    "# Define the words (notice that we include both meaning and grammar)\n",
    "\n",
    "# nouns\n",
    "man, woman, kid = Word('man', n), Word('woman', n), Word('kid', n)\n",
    "# adjectives\n",
    "morose, irascible = Word('morose', n @ n.l), Word('irascible', n @ n.l)\n",
    "frightened, cheerful = Word('frightened', n @ n.l), Word('cheerful', n @ n.l)\n",
    "gloomy, furious = Word('gloomy', n @ n.l), Word('furious', n @ n.l)\n",
    "terrified, joyful = Word('terrified', n @ n.l), Word('joyful', n @ n.l)\n",
    "content, jolly = Word('content', n @ n.l), Word('jolly', n @ n.l)\n",
    "downcast, miserable = Word('downcast', n @ n.l), Word('miserable', n @ n.l)\n",
    "mad, angered = Word('mad', n @ n.l), Word('angered', n @ n.l)\n",
    "afraid, horrified = Word('afraid', n @ n.l), Word('horrified', n @ n.l)\n",
    "old, young = Word('old', n @ n.l), Word('young', n @ n.l)\n",
    "# Intransitive verbs\n",
    "cries, shouts = Word('cries', n.r @ s), Word('shouts', n.r @ s)\n",
    "laughs, snaps = Word('laughs', n.r @ s), Word('snaps', n.r @ s)\n",
    "# Transitive verbs\n",
    "grieves, startles = Word('grieves', n.r @ s @ n.l), Word('startles', n.r @ s @ n.l)\n",
    "entertains, irritates = Word('entertains', n.r @ s @ n.l), Word('irritates', n.r @ s @ n.l)\n",
    "\n",
    "nouns = [man, woman, kid]\n",
    "adjectives = [morose, irascible, frightened, cheerful, gloomy, furious, terrified, joyful, old, young,\n",
    "             content, jolly, downcast, miserable, mad, angered, afraid, horrified]\n",
    "int_verbs = [cries, shouts, laughs, snaps]\n",
    "t_verbs = [grieves, startles, entertains, irritates]\n",
    "\n",
    "vocab = nouns + int_verbs + t_verbs + adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-wrapping",
   "metadata": {},
   "source": [
    "### 1.2. Define the grammar\n",
    "\n",
    "In this dataset we are going to consider the following structures to construct the sentences:\n",
    "\n",
    "- adj + noun + int. verb\n",
    "- noun + t. verb + noun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-vienna",
   "metadata": {},
   "source": [
    "- Intransitive sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "automotive-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the grammatical structures in a dictionary \n",
    "\n",
    "grammar_dict = {\n",
    "    'ADJ_N_IV' : Id(n) @ Cup(n.l, n) @ Id(n.r @ s) >> Cup(n, n.r) @ Id(s) >> Id(s),\n",
    "    'N_TV_N': Cup(n, n.r) @ Id(s) @ Cup(n.l, n),\n",
    "    'ADJ_N_TV_N': Id(n) @ Cup(n.l, n) @ Id(n.r @ s) @ Cup(n.l, n) >> Cup(n, n.r) @ Id(s) >> Id(s),\n",
    "    'N_TV_ADJ_N': Cup(n, n.r) @ Id(s) @ Cup(n.l, n) @ Cup(n.l, n) >> Id(s)}\n",
    "                                                                          \n",
    "\n",
    "# Create parsing (grammatical analysis) dictionary where the grammatical sentences\n",
    "# are the keys and the associated values are the diagrams (words + grammar)\n",
    "\n",
    "data_psr = {}\n",
    "\n",
    "# Intransitive sentences\n",
    "parsing_int = {\"{} {} {}.\".format(adj, noun, int_verb): adj @ noun @ int_verb >> grammar_dict['ADJ_N_IV']\n",
    "            for adj in adjectives for noun in nouns for int_verb in int_verbs}\n",
    "sentences_int = list(parsing_int.keys())\n",
    "for sentence in sentences_int:\n",
    "    diagram = parsing_int[sentence]\n",
    "    data_psr[sentence] = parsing_int[sentence]\n",
    "\n",
    "# Transitive sentences (without adjective)\n",
    "parsing_tra = {\"{} {} {}.\".format(subj, t_verb, obj):  subj @ t_verb @ obj >> grammar_dict['N_TV_N']\n",
    "            for subj in nouns for t_verb in t_verbs for obj in nouns}\n",
    "\n",
    "\n",
    "# Transitive sentences (with adjective)\n",
    "parsing_tra_ladj = {\"{} {} {} {}.\".format(adj, subj, t_verb, obj):  adj @ subj @ t_verb @ obj >> grammar_dict['ADJ_N_TV_N']\n",
    "            for adj in adjectives for subj in nouns for t_verb in t_verbs for obj in nouns}\n",
    "parsing_tra_radj = {\"{} {} {} {}.\".format(subj, t_verb, adj, obj):  subj @ t_verb @ adj @ obj >> grammar_dict['N_TV_ADJ_N']\n",
    "            for subj in nouns for t_verb in t_verbs for adj in adjectives for obj in nouns}\n",
    "\n",
    "parsing_tra.update(parsing_tra_ladj) #merges transitive adjectives into original dict\n",
    "parsing_tra.update(parsing_tra_radj)\n",
    "\n",
    "sentences_tra = list(parsing_tra.keys())\n",
    "for sentence in sentences_tra:\n",
    "    diagram = parsing_tra[sentence]\n",
    "    data_psr[sentence] = parsing_tra[sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-collective",
   "metadata": {},
   "source": [
    "Now, we have the dataset with the sentences and their corresponding meaning and grammar. The next step is to design the corresponding quantum circuits to determine the sentiment for each sentence. We are aiming to distinguish between four different emotions: happy (0), sad (1), angry (2), scared (3). However, some sentences of the dataset cannot be clearly classified according to this criteria (for example, 'Old man cries'). Therefore, the next step is to manually modify the dataset so all the sentences can be classified according to this criteria. In order to do that we will create a .txt file and assign the corresponding class to the viable sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-amplifier",
   "metadata": {},
   "source": [
    "### 1.3. Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "operational-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sentiment_large_corrected_balanced.txt') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "negative-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {}\n",
    "data_psr_dict = {}\n",
    "sent_type = {}\n",
    "for sentence in data:\n",
    "    sentstr = sentence[:-7]\n",
    "    if sentence[-6:-3] == 'int':\n",
    "        diagram = parsing_int[sentstr]\n",
    "        data_psr_dict[sentstr] = diagram\n",
    "        labels_dict[sentstr] = sentence[-2]\n",
    "        sent_type[sentstr] = 'int'\n",
    "    elif sentence[-6:-3] == 'tra':\n",
    "        diagram = parsing_tra[sentstr]\n",
    "        data_psr_dict[sentstr] = diagram\n",
    "        labels_dict[sentstr] = sentence[-2]\n",
    "        sent_type[sentstr] = 'tra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bronze-modern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements for each sentiment\n",
      "Happy:  90\n",
      "Sad:  96\n",
      "Angry:  92\n",
      "Scared:  87\n",
      "Total 365\n"
     ]
    }
   ],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "c3 = 0\n",
    "for value in labels_dict.values():\n",
    "    if value == '0':\n",
    "        c0 +=1\n",
    "    elif value == '1':\n",
    "        c1 += 1\n",
    "    elif value == '2':\n",
    "        c2 += 1\n",
    "    elif value == '3':\n",
    "        c3 += 1\n",
    "print('Number of elements for each sentiment')\n",
    "print('Happy: ', c0)\n",
    "print('Sad: ', c1)\n",
    "print('Angry: ', c2)\n",
    "print('Scared: ', c3)\n",
    "print('Total', len(data_psr_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-paradise",
   "metadata": {},
   "source": [
    "Now we have our dataset! The only problem left is the fact that the cups used in the diagrams are too resource consumming. Luckily, it is possible to remove them by transforming the states into effects (we are just doing this with the nouns as in https://github.com/CQCL/qnlp_lorenz_etal_2021_resources). Let us see some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-matthew",
   "metadata": {},
   "source": [
    "Let us apply this to our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6883adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_psr_dict = {}\n",
    "for sentstr in data_psr_dict.keys():\n",
    "    num_words = len(sentstr.split(' '))\n",
    "    words = data_psr_dict[sentstr].boxes[:num_words]\n",
    "    if sent_type[sentstr] == 'int':\n",
    "        noun = Box(words[1].name, n.l, Ty())\n",
    "        words_new = (Cap(n, n.l)) >> (words[0] @ Id(n) @ noun @ words[2])\n",
    "        sentence = words_new >> grammar_dict['ADJ_N_IV']\n",
    "        data_new_psr_dict[sentstr] = sentence.normal_form()\n",
    "    elif sent_type[sentstr] == 'tra':\n",
    "        if num_words == 3:\n",
    "            noun1 = Box(words[0].name, n.r, Ty())\n",
    "            noun2 = Box(words[2].name, n.l, Ty())\n",
    "            words_new = (Cap(n.r, n) @ Cap(n, n.l)) >> (noun1 @ Id(n) @ words[1] @ Id(n) @ noun2)\n",
    "            sentence = words_new >> grammar_dict['N_TV_N']\n",
    "            data_new_psr_dict[sentstr] = sentence.normal_form()\n",
    "        elif words[0] in adjectives: #adjective at beginning\n",
    "            noun1 = Box(words[1].name, n.l, Ty())\n",
    "            noun2 = Box(words[3].name, n.l, Ty())\n",
    "            words_new = (Cap(n, n.l) @ Cap(n, n.l)) >> (words[0] @ Id(n) @ noun1 @ words[2] @ Id(n) @ noun2)\n",
    "            sentence = words_new >> grammar_dict['ADJ_N_TV_N']\n",
    "            data_new_psr_dict[sentstr] = sentence.normal_form()\n",
    "        else: #adjective on second noun\n",
    "            noun1 = Box(words[0].name, n.r, Ty())\n",
    "            noun2 = Box(words[3].name, n.l, Ty())\n",
    "            words_new = (Cap(n.r, n) @ Cap(n, n.l)) >> (noun1 @ Id(n) @ words[1] @ words[2] @ Id(n) @ noun2)\n",
    "            sentence = words_new >> grammar_dict['N_TV_ADJ_N']\n",
    "            data_new_psr_dict[sentstr] = sentence.normal_form() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-request",
   "metadata": {},
   "source": [
    "The final step before the implementation of the quantum circuit is to redefine the vocabulary according to the new domain and codamain for the nouns as effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decent-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_psr = []\n",
    "for word in vocab:\n",
    "    if word.cod == Ty('n'):\n",
    "        vocab_psr.append(Box(word.name, n.r, Ty()))   # n.l case is dealt with in definition of quantum functor\n",
    "    else:\n",
    "        vocab_psr.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-manhattan",
   "metadata": {},
   "source": [
    "## 2. Create quantum circuit\n",
    "\n",
    "Once the dataset and its corresponding diagrams are created, the next step is to construct the variational quantum circuits associated with them. In order to do that, we will use different ansätze depending on the type of the word that we want to represent. In this case we only have two types of words, nouns and verbs. Both types will have associated 2 qubits (as we have four sentiments, we need 4 quantum states to encode the result of the classification). Moreover, we will also distinguish between states and effects when constructing the ansätze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crude-philippines",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# Translation to quantum circuits\n",
    "#*****************************************************\n",
    "from discopy.quantum import Ket, IQPansatz, Bra\n",
    "from discopy.quantum.gates import sqrt, H, CZ, Rz, Rx, CX\n",
    "from discopy.quantum.circuit import CircuitFunctor, Id\n",
    "from discopy.quantum.circuit import Circuit as DCP_Circuit\n",
    "\n",
    "ob = {s: q_s, n: q_n}                           # assignment of number of qubits to atomic grammatical types\n",
    "ob_cqmap = {s: qubit ** q_s, n: qubit ** q_n}   # the form in which it is needed for discopy's cqmap module\n",
    "\n",
    "#-----------------------------------------\n",
    "# parametrised part of ansaetze\n",
    "#-----------------------------------------\n",
    "\n",
    "def single_qubit_iqp_ansatz(params):\n",
    "    if len(params) == 1:\n",
    "        return Rx(params[0])  \n",
    "    if len(params) == 2:\n",
    "        return Rx(params[0]) >> Rz(params[1])\n",
    "    if len(params) == 3:\n",
    "        return IQPansatz(1, params)       \n",
    "\n",
    "def ansatz_state(state, params):  \n",
    "    # Obtain the number of qubits for a given state summing the corresponding number of qubit to each factor\n",
    "    # of the codomain using its Type\n",
    "    arity = sum(ob[Ty(factor.name)] for factor in state.cod) \n",
    "    if arity == 1:\n",
    "        return Ket(0) >> single_qubit_iqp_ansatz(params)\n",
    "    else:\n",
    "        return Ket(*tuple([0 for i in range(arity)])) >> IQPansatz(arity, params)\n",
    "    \n",
    "def ansatz_effect(effect, params):  \n",
    "    arity = sum(ob[Ty(factor.name)] for factor in effect.dom)\n",
    "    if arity == 1:\n",
    "        return single_qubit_iqp_ansatz(params) >> Bra(0)\n",
    "    else:\n",
    "        return IQPansatz(arity, params) >> Bra(*tuple([0 for i in range(arity)]))\n",
    "       \n",
    "def ansatz(box,params):\n",
    "    dom_type = box.dom\n",
    "    cod_type = box.cod\n",
    "    if len(dom_type) == 0 and len(cod_type) != 0:\n",
    "        return ansatz_state(box, params)\n",
    "    if len(dom_type) != 0 and len(cod_type) == 0: # Box is a noun (effect)\n",
    "        return ansatz_effect(box, params)\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Define parametrised functor to quantum circuits\n",
    "#----------------------------------------------------------\n",
    "def F(params): \n",
    "    ar = dict()\n",
    "    for i in range(len(vocab_psr)):\n",
    "        pgbox = vocab_psr[i]\n",
    "        qbox = ansatz(vocab_psr[i], params[i])\n",
    "        ar.update({pgbox: qbox})\n",
    "        if pgbox.cod == Ty():\n",
    "            ar.update({Box(pgbox.name, n.l, Ty()): qbox})  # send the effect with n.l to same quantum effect\n",
    "    return CircuitFunctor(ob_cqmap, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intimate-relationship",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# Functions to deal with the parametrisation\n",
    "#*****************************************************\n",
    "\n",
    "def paramshapes(vocab_psr):\n",
    "    parshapes = []    \n",
    "    for box in vocab_psr:\n",
    "        dom_type = box.dom\n",
    "        cod_type = box.cod\n",
    "        dom_arity = sum(ob[Ty(factor.name)] for factor in box.dom)\n",
    "        cod_arity = sum(ob[Ty(factor.name)] for factor in box.cod)\n",
    "        if dom_arity == 0 or cod_arity == 0:  # states and effects\n",
    "            arity = max(dom_arity, cod_arity)\n",
    "            assert arity != 0\n",
    "            if arity == 1:\n",
    "                parshapes.append((p_n,))       \n",
    "            if arity != 1:\n",
    "                parshapes.append((depth, arity-1))\n",
    "    return parshapes\n",
    "\n",
    "def randparams(par_shapes):\n",
    "    params = np.array([]) \n",
    "    for i in range(len(par_shapes)):\n",
    "        params = np.concatenate((params, np.ravel(np.random.rand(*par_shapes[i])))) # np.ravel flattens an array\n",
    "    return params \n",
    "\n",
    "def reshape_params(unshaped_pars, par_shapes):\n",
    "    pars_reshaped = [[] for ii in range(len(par_shapes))]\n",
    "    shift = 0\n",
    "    for ss, s in enumerate(par_shapes):\n",
    "        idx0 = 0 + shift\n",
    "        if len(s) == 1:\n",
    "            idx1 = s[0] + shift\n",
    "        elif len(s) == 2:\n",
    "            idx1 = s[0] * s[1] + shift\n",
    "        pars_reshaped[ss] = np.reshape(unshaped_pars[idx0:idx1], s)\n",
    "        if len(s) == 1:\n",
    "            shift += s[0]\n",
    "        elif len(s) == 2:\n",
    "            shift += s[0] * s[1]\n",
    "    return pars_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "medieval-transcription",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:     39\n"
     ]
    }
   ],
   "source": [
    "#****************************************\n",
    "# Parameters of the current model\n",
    "#****************************************\n",
    "\n",
    "par_shapes = paramshapes(vocab_psr)\n",
    "rand_unshaped_pars = randparams(par_shapes)\n",
    "rand_shaped_pars = reshape_params(rand_unshaped_pars, par_shapes)\n",
    "\n",
    "print('Number of parameters:    ', len(rand_unshaped_pars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-aerospace",
   "metadata": {},
   "source": [
    "## 3. Create training and test dataset\n",
    "\n",
    "The next step is to divide our dataset into training and test data, so we can perform the classification using a supervised quantum machine learning technique. We need the data, which are the quantum circuits associated to each sentences, and the labels, that encode the sentiment. In this case the labels are the four possible quantum states that can be obtained from measuring a 2-qubit quantum circuit:\n",
    "\n",
    "- Happy: $ \\ 0 \\ \\rightarrow \\ |00\\rangle = [1,0,0,0] \\ \\rightarrow \\ p_{00}=1$,\n",
    "- Sad: $ \\ 1 \\ \\rightarrow \\ |01\\rangle = [0,1,0,0] \\ \\rightarrow \\ p_{01}=1$,\n",
    "- Angry: $ \\ 2 \\ \\rightarrow \\ |10\\rangle = [0,0,1,0] \\ \\rightarrow \\ p_{10}=1$,\n",
    "- Scared: $ \\ 3 \\ \\rightarrow \\ |11\\rangle = [0,0,0,1] \\ \\rightarrow \\ p_{11}=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8cc90f-d08a-4e8a-830e-899d3f25cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "psr_diagrams = []\n",
    "psr_diagrams_dict = {}\n",
    "psr_labels = []\n",
    "sentences = []\n",
    "\n",
    "for sentstr in data_new_psr_dict.keys():\n",
    "    sentences.append(sentstr)\n",
    "    diagram = data_new_psr_dict[sentstr]\n",
    "    psr_diagrams.append(diagram)\n",
    "    psr_diagrams_dict[sentstr] = diagram\n",
    "    if labels_dict[sentstr] == '0':\n",
    "        label = np.array([1,0])\n",
    "    elif labels_dict[sentstr] == '1':\n",
    "        label = np.array([0,1])\n",
    "    psr_labels.append(label)\n",
    "\n",
    "train_circuits_pg_psr, test_circuits_pg_psr, train_labels, test_labels = \\\n",
    "    train_test_split(psr_diagrams, psr_labels, test_size=0.25, random_state=42)\n",
    "train_sent, test_sent, train_labels_sent, test_labels_sent = \\\n",
    "    train_test_split(sentences, psr_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "845dd605-d925-4561-a20b-5900dca28793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "psr_diagrams = []\n",
    "psr_diagrams_dict = {}\n",
    "psr_labels = []\n",
    "sentences = []\n",
    "\n",
    "for sentstr in data_new_psr_dict.keys():\n",
    "    sentences.append(sentstr)\n",
    "    diagram = data_new_psr_dict[sentstr]\n",
    "    psr_diagrams.append(diagram)\n",
    "    psr_diagrams_dict[sentstr] = diagram\n",
    "    label = int(labels_dict[sentstr])\n",
    "    psr_labels.append(label)\n",
    "\n",
    "train_data_psr, test_data_psr, orig_train_labels, test_labels = \\\n",
    "    train_test_split(psr_diagrams, psr_labels, test_size=0.25, random_state=42)\n",
    "train_sent, test_sent, train_labels_sent, test_labels_sent = \\\n",
    "    train_test_split(sentences, psr_labels, test_size=0.25, random_state=42)\n",
    "    \n",
    "train_happy = []\n",
    "train_sad = []\n",
    "train_angry = []\n",
    "train_scared = []\n",
    "\n",
    "for i, data in enumerate(train_data_psr):\n",
    "    if orig_train_labels[i] == 0:\n",
    "        train_happy.append(data)\n",
    "    elif orig_train_labels[i] == 1:\n",
    "        train_sad.append(data)\n",
    "    elif orig_train_labels[i] == 2:\n",
    "        train_angry.append(data)\n",
    "    elif orig_train_labels[i] == 3:\n",
    "        train_scared.append(data)\n",
    "        \n",
    "import random\n",
    "# Random seed\n",
    "seed = np.random.randint(1000)\n",
    "seed = 0\n",
    "# Happy vs sad\n",
    "train_happy_vs_sad = train_happy + train_sad\n",
    "labels_happy_vs_sad = [[1,0]]*len(train_happy) + [[0,1]]*len(train_sad)\n",
    "random.Random(seed).shuffle(train_happy_vs_sad)\n",
    "random.Random(seed).shuffle(labels_happy_vs_sad)\n",
    "# Happy vs angry\n",
    "train_happy_vs_angry = train_happy + train_angry\n",
    "labels_happy_vs_angry = [[1,0]]*len(train_happy) + [[0,1]]*len(train_angry)\n",
    "random.Random(seed).shuffle(train_happy_vs_angry)\n",
    "random.Random(seed).shuffle(labels_happy_vs_angry)\n",
    "# Happy vs scared\n",
    "train_happy_vs_scared = train_happy + train_scared\n",
    "labels_happy_vs_scared = [[1,0]]*len(train_happy) + [[0,1]]*len(train_scared)\n",
    "random.Random(seed).shuffle(train_happy_vs_scared)\n",
    "random.Random(seed).shuffle(labels_happy_vs_scared)\n",
    "# Sad vs angry\n",
    "train_sad_vs_angry = train_sad + train_angry\n",
    "labels_sad_vs_angry = [[1,0]]*len(train_sad) + [[0,1]]*len(train_angry)\n",
    "random.Random(seed).shuffle(train_sad_vs_angry)\n",
    "random.Random(seed).shuffle(labels_sad_vs_angry)\n",
    "# Sad vs scared\n",
    "train_sad_vs_scared = train_sad + train_scared\n",
    "labels_sad_vs_scared = [[1,0]]*len(train_sad) + [[0,1]]*len(train_scared)\n",
    "random.Random(seed).shuffle(train_sad_vs_scared)\n",
    "random.Random(seed).shuffle(labels_sad_vs_scared)\n",
    "# Angry vs scared\n",
    "train_angry_vs_scared = train_angry + train_scared\n",
    "labels_angry_vs_scared = [[1,0]]*len(train_angry) + [[0,1]]*len(train_scared)\n",
    "random.Random(seed).shuffle(train_angry_vs_scared)\n",
    "random.Random(seed).shuffle(labels_angry_vs_scared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6c3c9e6-4955-47aa-9e08-39fb836f1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_circuits_pg_psr = train_happy_vs_sad\n",
    "train_labels = labels_happy_vs_sad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-instrument",
   "metadata": {},
   "source": [
    "## 4. Qiskit SPSA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "variable-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2a32a74-1505-4bd2-bcda-c2543626f178",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#**********************************************************************************\n",
    "# Split cost and error functions for time efficiency\n",
    "#**********************************************************************************\n",
    "def get_cost(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    train_circuits = [(func(circ) >> Measure()) for circ in train_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*train_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "    results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in results]\n",
    "    pred_labels_distrs = [res / np.sum(res) for res in results_tweaked]\n",
    "    cross_entropies = np.array([np.sum(train_labels[s] * np.log2(pred_labels_distrs[s])) for s in range(len(train_labels))])\n",
    "    return -1 / len(train_circuits_pg_psr) * np.sum(cross_entropies)\n",
    "\n",
    "def get_train_error(pred_labels_distrs):\n",
    "    error = 0.0\n",
    "    assert len(pred_labels_distrs[0]) == 2  # rounding only makes sense if labels are binary tuples\n",
    "    pred_labels = [np.round(res) for res in pred_labels_distrs]\n",
    "    for i in range(len(pred_labels)):\n",
    "        if np.sum(pred_labels[i]) != 1.0:  # when equal weights or no counts gives label [1,1] (due to - 1e-9)\n",
    "            error += 1\n",
    "        else:\n",
    "            error += np.abs(train_labels[i][0] - pred_labels[i][0]) # above ensures precited label as [0,1] or [1,0]\n",
    "    return round(error * 100 / len(train_circuits_pg_psr), 1)\n",
    "\n",
    "def get_dev_error(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    dev_circuits = [(func(circ) >> Measure()) for circ in dev_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*dev_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "    results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in results]\n",
    "    assert len(results_tweaked[0]) == 2\n",
    "    pred_labels = [np.round(res / np.sum(res)) for res in results_tweaked]\n",
    "    error = 0.0\n",
    "    for i in range(len(pred_labels)):\n",
    "        if np.sum(pred_labels[i]) != 1.0:\n",
    "            error += 1\n",
    "        else:\n",
    "            error += np.abs(dev_labels[i][0] - pred_labels[i][0])\n",
    "    error = round(error * 100 / len(dev_data), 1)\n",
    "    return error, pred_labels\n",
    "\n",
    "def get_test_error(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    test_circuits = [(func(circ) >> Measure()) for circ in test_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*test_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "    results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in results]\n",
    "    assert len(results_tweaked[0]) == 2\n",
    "    pred_labels = [np.round(res / np.sum(res)) for res in results_tweaked]\n",
    "    error = 0.0\n",
    "    for i in range(len(pred_labels)):\n",
    "        if np.sum(pred_labels[i]) != 1.0:\n",
    "            error += 1\n",
    "        else:\n",
    "            error += np.abs(test_labels[i][0] - pred_labels[i][0])\n",
    "    error = round(error * 100 / len(test_circuits_pg_psr), 1)\n",
    "    return error, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "918c53c6-74e2-494b-81d0-fb86d90a8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is part of Qiskit.\n",
    "#\n",
    "# (C) Copyright IBM 2018, 2021.\n",
    "#\n",
    "# This code is licensed under the Apache License, Version 2.0. You may\n",
    "# obtain a copy of this license in the LICENSE.txt file in the root directory\n",
    "# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "#\n",
    "# Any modifications or derivative works of this code must retain this\n",
    "# copyright notice, and modified files need to carry a notice indicating\n",
    "# that they have been altered from the originals.\n",
    "\n",
    "\"\"\"Simultaneous Perturbation Stochastic Approximation optimizer.\"\"\"\n",
    "\n",
    "import warnings\n",
    "from typing import Optional, List, Callable\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from qiskit.aqua import aqua_globals\n",
    "from qiskit.aqua.utils.validation import validate_min\n",
    "from optimizer import Optimizer, OptimizerSupportLevel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SPSA(Optimizer):\n",
    "    \"\"\"\n",
    "    Simultaneous Perturbation Stochastic Approximation (SPSA) optimizer.\n",
    "\n",
    "    SPSA is an algorithmic method for optimizing systems with multiple unknown parameters.\n",
    "    As an optimization method, it is appropriately suited to large-scale population models,\n",
    "    adaptive modeling, and simulation optimization.\n",
    "\n",
    "    .. seealso::\n",
    "        Many examples are presented at the `SPSA Web site <http://www.jhuapl.edu/SPSA>`__.\n",
    "\n",
    "    SPSA is a descent method capable of finding global minima,\n",
    "    sharing this property with other methods as simulated annealing.\n",
    "    Its main feature is the gradient approximation, which requires only two\n",
    "    measurements of the objective function, regardless of the dimension of the optimization\n",
    "    problem.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        SPSA can be used in the presence of noise, and it is therefore indicated in situations\n",
    "        involving measurement uncertainty on a quantum computation when finding a minimum.\n",
    "        If you are executing a variational algorithm using a Quantum ASseMbly Language (QASM)\n",
    "        simulator or a real device, SPSA would be the most recommended choice among the optimizers\n",
    "        provided here.\n",
    "\n",
    "    The optimization process includes a calibration phase, which requires additional\n",
    "    functional evaluations.\n",
    "\n",
    "    For further details, please refer to https://arxiv.org/pdf/1704.05018v2.pdf#section*.11\n",
    "    (Supplementary information Section IV.)\n",
    "    \"\"\"\n",
    "\n",
    "    _C0 = 2 * np.pi * 0.1\n",
    "    _OPTIONS = ['save_steps', 'last_avg']\n",
    "\n",
    "    # pylint: disable=unused-argument\n",
    "    def __init__(self,\n",
    "                 maxiter: int = 1000,\n",
    "                 save_steps: int = 1,\n",
    "                 last_avg: int = 1,\n",
    "                 c0: float = _C0,\n",
    "                 c1: float = 0.1,\n",
    "                 c2: float = 0.602,\n",
    "                 c3: float = 0.101,\n",
    "                 c4: float = 0,\n",
    "                 skip_calibration: bool = False,\n",
    "                 max_trials: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            maxiter: Maximum number of iterations to perform.\n",
    "            save_steps: Save intermediate info every save_steps step. It has a min. value of 1.\n",
    "            last_avg: Averaged parameters over the last_avg iterations.\n",
    "                If last_avg = 1, only the last iteration is considered. It has a min. value of 1.\n",
    "            c0: The initial a. Step size to update parameters.\n",
    "            c1: The initial c. The step size used to approximate gradient.\n",
    "            c2: The alpha in the paper, and it is used to adjust a (c0) at each iteration.\n",
    "            c3: The gamma in the paper, and it is used to adjust c (c1) at each iteration.\n",
    "            c4: The parameter used to control a as well.\n",
    "            skip_calibration: Skip calibration and use provided c(s) as is.\n",
    "            max_trials: Deprecated, use maxiter.\n",
    "        \"\"\"\n",
    "        validate_min('save_steps', save_steps, 1)\n",
    "        validate_min('last_avg', last_avg, 1)\n",
    "        super().__init__()\n",
    "        if max_trials is not None:\n",
    "            warnings.warn('The max_trials parameter is deprecated as of '\n",
    "                          '0.8.0 and will be removed no sooner than 3 months after the release. '\n",
    "                          'You should use maxiter instead.',\n",
    "                          DeprecationWarning)\n",
    "            maxiter = max_trials\n",
    "        for k, v in list(locals().items()):\n",
    "            if k in self._OPTIONS:\n",
    "                self._options[k] = v\n",
    "        self._maxiter = maxiter\n",
    "        self._parameters = np.array([c0, c1, c2, c3, c4])\n",
    "        self._skip_calibration = skip_calibration\n",
    "\n",
    "\n",
    "    def get_support_level(self):\n",
    "        \"\"\" return support level dictionary \"\"\"\n",
    "        return {\n",
    "            'gradient': OptimizerSupportLevel.ignored,\n",
    "            'bounds': OptimizerSupportLevel.ignored,\n",
    "            'initial_point': OptimizerSupportLevel.required\n",
    "        }\n",
    "\n",
    "\n",
    "    def optimize(self, num_vars, objective_function, gradient_function=None,\n",
    "                 variable_bounds=None, initial_point=None):\n",
    "        super().optimize(num_vars, objective_function, gradient_function,\n",
    "                         variable_bounds, initial_point)\n",
    "\n",
    "        if not isinstance(initial_point, np.ndarray):\n",
    "            initial_point = np.asarray(initial_point)\n",
    "\n",
    "        logger.debug('Parameters: %s', self._parameters)\n",
    "        if not self._skip_calibration:\n",
    "            # at least one calibration, at most 25 calibrations\n",
    "            num_steps_calibration = min(25, max(1, self._maxiter // 5))\n",
    "            self._calibration(objective_function, initial_point, num_steps_calibration)\n",
    "        else:\n",
    "            logger.debug('Skipping calibration, parameters used as provided.')\n",
    "\n",
    "        opt, sol, cost_plus_save, cost_minus_save, theta_plus_save, theta_minus_save = self._optimization(objective_function,\n",
    "                                                  initial_point,\n",
    "                                                  maxiter=self._maxiter,\n",
    "                                                  **self._options)\n",
    "        return sol, opt, cost_plus_save, cost_minus_save,theta_plus_save, theta_minus_save\n",
    "\n",
    "\n",
    "    def _optimization(self,\n",
    "                      obj_fun: Callable,\n",
    "                      initial_theta: np.ndarray,\n",
    "                      maxiter: int,\n",
    "                      save_steps: int = 1,\n",
    "                      last_avg: int = 1) -> List:\n",
    "        \"\"\"Minimizes obj_fun(theta) with a simultaneous perturbation stochastic\n",
    "        approximation algorithm.\n",
    "\n",
    "        Args:\n",
    "            obj_fun: the function to minimize\n",
    "            initial_theta: initial value for the variables of obj_fun\n",
    "            maxiter: the maximum number of trial steps ( = function\n",
    "                calls/2) in the optimization\n",
    "            save_steps: stores optimization outcomes each 'save_steps'\n",
    "                trial steps\n",
    "            last_avg: number of last updates of the variables to average\n",
    "                on for the final obj_fun\n",
    "        Returns:\n",
    "            a list with the following elements:\n",
    "                cost_final : final optimized value for obj_fun\n",
    "                theta_best : final values of the variables corresponding to\n",
    "                    cost_final\n",
    "                cost_plus_save : array of stored values for obj_fun along the\n",
    "                    optimization in the + direction\n",
    "                cost_minus_save : array of stored values for obj_fun along the\n",
    "                    optimization in the - direction\n",
    "                theta_plus_save : array of stored variables of obj_fun along the\n",
    "                    optimization in the + direction\n",
    "                theta_minus_save : array of stored variables of obj_fun along the\n",
    "                    optimization in the - direction\n",
    "        \"\"\"\n",
    "        print('Hello optimization')\n",
    "        theta_plus_save = []\n",
    "        theta_minus_save = []\n",
    "        cost_plus_save = []\n",
    "        cost_minus_save = []\n",
    "        theta = initial_theta\n",
    "        theta_best = np.zeros(initial_theta.shape)\n",
    "        for k in range(maxiter):\n",
    "            print(k)\n",
    "            # SPSA Parameters\n",
    "            a_spsa = float(self._parameters[0]) / np.power(k + 1 + self._parameters[4],\n",
    "                                                           self._parameters[2])\n",
    "            c_spsa = float(self._parameters[1]) / np.power(k + 1, self._parameters[3])\n",
    "            delta = 2 * aqua_globals.random.integers(2, size=np.shape(initial_theta)[0]) - 1\n",
    "            # plus and minus directions\n",
    "            theta_plus = theta + c_spsa * delta\n",
    "            theta_minus = theta - c_spsa * delta\n",
    "            # cost function for the two directions\n",
    "            if self._max_evals_grouped > 1:\n",
    "                cost_plus, cost_minus = obj_fun(np.concatenate((theta_plus, theta_minus)))\n",
    "            else:\n",
    "                cost_plus = obj_fun(theta_plus)\n",
    "                cost_minus = obj_fun(theta_minus)\n",
    "            # derivative estimate\n",
    "            g_spsa = (cost_plus - cost_minus) * delta / (2.0 * c_spsa)\n",
    "            # updated theta\n",
    "            theta = theta - a_spsa * g_spsa\n",
    "            # saving\n",
    "            if k % save_steps == 0:\n",
    "                logger.debug('Objective function at theta+ for step # %s: %1.7f', k, cost_plus)\n",
    "                logger.debug('Objective function at theta- for step # %s: %1.7f', k, cost_minus)\n",
    "                theta_plus_save.append(theta_plus)\n",
    "                theta_minus_save.append(theta_minus)\n",
    "                cost_plus_save.append(cost_plus)\n",
    "                cost_minus_save.append(cost_minus)\n",
    "\n",
    "            if k >= maxiter - last_avg:\n",
    "                theta_best += theta / last_avg\n",
    "        # final cost update\n",
    "        cost_final = obj_fun(theta_best)\n",
    "        logger.debug('Final objective function is: %.7f', cost_final)\n",
    "\n",
    "        return [cost_final, theta_best, cost_plus_save, cost_minus_save,\n",
    "                theta_plus_save, theta_minus_save]\n",
    "\n",
    "    def _calibration(self,\n",
    "                     obj_fun: Callable,\n",
    "                     initial_theta: np.ndarray,\n",
    "                     stat: int):\n",
    "        \"\"\"Calibrates and stores the SPSA parameters back.\n",
    "\n",
    "        SPSA parameters are c0 through c5 stored in parameters array\n",
    "\n",
    "        c0 on input is target_update and is the aimed update of variables on the first trial step.\n",
    "        Following calibration c0 will be updated.\n",
    "\n",
    "        c1 is initial_c and is first perturbation of initial_theta.\n",
    "\n",
    "        Args:\n",
    "            obj_fun: the function to minimize.\n",
    "            initial_theta: initial value for the variables of obj_fun.\n",
    "            stat: number of random gradient directions to average on in the calibration.\n",
    "        \"\"\"\n",
    "        print('Hello callibration')\n",
    "        target_update = self._parameters[0]\n",
    "        initial_c = self._parameters[1]\n",
    "        delta_obj = 0\n",
    "        logger.debug(\"Calibration...\")\n",
    "        for i in range(stat):\n",
    "            if i % 5 == 0:\n",
    "                logger.debug('calibration step # %s of %s', str(i), str(stat))\n",
    "            delta = 2 * aqua_globals.random.integers(2, size=np.shape(initial_theta)[0]) - 1\n",
    "            theta_plus = initial_theta + initial_c * delta\n",
    "            theta_minus = initial_theta - initial_c * delta\n",
    "            if self._max_evals_grouped > 1:\n",
    "                obj_plus, obj_minus = obj_fun(np.concatenate((theta_plus, theta_minus)))\n",
    "            else:\n",
    "                obj_plus = obj_fun(theta_plus)\n",
    "                obj_minus = obj_fun(theta_minus)\n",
    "            delta_obj += np.absolute(obj_plus - obj_minus) / stat\n",
    "\n",
    "        # only calibrate if delta_obj is larger than 0\n",
    "        if delta_obj > 0:\n",
    "            self._parameters[0] = target_update * 2 / delta_obj \\\n",
    "                * self._parameters[1] * (self._parameters[4] + 1)\n",
    "            logger.debug('delta_obj is 0, not calibrating (since this would set c0 to inf)')\n",
    "\n",
    "        logger.debug('Calibrated SPSA parameter c0 is %.7f', self._parameters[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d77dbc35-2f89-49cf-befd-60e5009f088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello optimization\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "opt = SPSA(1,skip_calibration=True)\n",
    "num_vars = len(rand_unshaped_pars)\n",
    "objective_function = get_cost\n",
    "initial_point = rand_unshaped_pars\n",
    "cost_final, theta_best, cost_plus_save, cost_minus_save, theta_plus_save, theta_minus_save = opt.optimize(num_vars, objective_function, \n",
    "             gradient_function=None, variable_bounds=None, initial_point=initial_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37a6b302-fec8-4bfa-a832-8c049954fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.75483984  3.39872321 -2.97819828 ... -2.74523258  4.11975414\n",
      "  3.51686279]\n",
      "1.3925452508372547\n",
      "[2.181559026944664]\n",
      "[1.1789414464544405]\n",
      "[array([0.50502381, 0.14890719, 0.27161775, ..., 0.50458345, 0.86993811,\n",
      "       0.26704676])]\n",
      "[array([0.70502381, 0.34890719, 0.07161775, ..., 0.30458345, 1.06993811,\n",
      "       0.46704676])]\n"
     ]
    }
   ],
   "source": [
    "print(cost_final)\n",
    "print(theta_best)\n",
    "print(cost_plus_save)\n",
    "print(cost_minus_save)\n",
    "print(theta_plus_save)\n",
    "print(theta_minus_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291804a-e661-4364-94af-e69e95cd429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = data['param_history'][-1]\n",
    "func = F(reshape_params(final_params, par_shapes))\n",
    "final_train_circuits = [(func(circ) >> Measure()) for circ in train_circuits_pg_psr]\n",
    "final_test_circuits = [(func(circ) >> Measure()) for circ in test_circuits_pg_psr]\n",
    "train_results = DCP_Circuit.eval(*final_train_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "test_results = DCP_Circuit.eval(*final_test_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "train_results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in train_results]\n",
    "test_results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in test_results]\n",
    "pred_train_results = [res.flatten() / np.sum(res) for res in train_results_tweaked]\n",
    "pred_test_results = [res.flatten() / np.sum(res) for res in test_results_tweaked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d86a63-ef92-4c62-bc2f-8537fbfff47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_train0 = 0\n",
    "correct_train1 = 0\n",
    "train0 = 0\n",
    "train1 = 0\n",
    "for i, res in enumerate(pred_train_results):\n",
    "    pred_result = np.argmax(res.flatten())\n",
    "    train_result = np.argmax(train_labels[i])\n",
    "    if train_result == 0:\n",
    "        train0 +=1\n",
    "    else:\n",
    "        train1 +=1\n",
    "    if train_result == pred_result:\n",
    "        correct += 1\n",
    "        if train_result == 0:\n",
    "            correct_train0 += 1\n",
    "        else:\n",
    "            correct_train1 += 1\n",
    "    #print(f'Result: {train_array, train_result}, Predicted result: {res, pred_result}')\n",
    "print('Correct predictions (train):',correct/len(train_results_tweaked))\n",
    "print('Correct predictions (train0):',correct_train0/train0)\n",
    "print('Correct predictions (train1):',correct_train1/train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc3c1c-ea76-42e0-b313-ddadac7a91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_test0 = 0\n",
    "correct_test1 = 0\n",
    "test0 = 0\n",
    "test1 = 0\n",
    "for i, res in enumerate(test_results_tweaked):\n",
    "    pred_result = np.argmax(res.flatten())\n",
    "    test_result = np.argmax(test_labels[i])\n",
    "    if test_result == 0:\n",
    "        test0 +=1\n",
    "    else:\n",
    "        test1 +=1\n",
    "    if test_result == pred_result:\n",
    "        correct += 1\n",
    "        if test_result == 0:\n",
    "            correct_test0 += 1\n",
    "        else:\n",
    "            correct_test1 += 1\n",
    "print('Correct predictions (test):',correct/len(test_results_tweaked))\n",
    "print('Correct predictions (test0):',correct_test0/test0)\n",
    "print('Correct predictions (test1):',correct_test1/test1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
